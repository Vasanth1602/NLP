{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c147d17-45eb-4d98-8ea2-cdd95c355b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ffcbc-f774-4a24-b474-89455706c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8f1a9-662d-44e4-81aa-ea0d87f246e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The children were playing happily in the gardens. \n",
    "They had been running and jumping for hours before it started raining. \n",
    "Later, they decided to go home and eat their favorite meals.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912d126-1c04-4212-8cde-10d4151ca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Sentence Segmentation\n",
    "# -------------------------------------------------------------\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"=== Sentence Segmentation ===\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"{i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5ce61-f460-42bf-bb64-96a772affbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Initialize Different Stemmers\n",
    "# -------------------------------------------------------------\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fac998-1f2d-49a0-b82a-f2f446856726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Stemming Results ===\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    words = word_tokenize(sentence)\n",
    "    porter_stems = [porter.stem(w) for w in words]\n",
    "    snow_stems = [snowball.stem(w) for w in words]\n",
    "    lanc_stems = [lancaster.stem(w) for w in words]\n",
    "\n",
    "\n",
    "    print(f\"\\nSentence {i}: {sentence}\")\n",
    "    print(\"Original Words :\", words)\n",
    "    print(\"Porter Stemmer :\", porter_stems)\n",
    "    print(\"Snowball Stemmer:\", snow_stems)\n",
    "    print(\"Lancaster Stemmer:\", lanc_stems)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
