{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a35d9-5570-4c58-9967-bba0b6853227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6ab8d-ed11-4944-8690-fc52916612de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')          # For tokenization\n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64a891-4f37-4a88-b8a0-772f083a7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The children were playing happily in the gardens. \n",
    "They had been running and jumping for hours before it started raining. \n",
    "Later, they decided to go home and eat their favorite meals.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f86cb-c7ca-41c6-a55f-4bce58c3be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization – splits text into sentences\n",
    "sent_tokens = sent_tokenize(text)\n",
    "\n",
    "# Word Tokenization – splits text into words\n",
    "word_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238caef9-c63a-4a23-ac2b-ab156afb723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Sentence Tokenization ===\")\n",
    "print(sent_tokens)\n",
    "print(\"\\n=== Word Tokenization ===\")\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d351b-56a7-45d7-907c-b91ed7002218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafb249-6d34-40d2-b901-f05ad72465a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to each token\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a9869-529c-4d99-8232-d1f0b3e61083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display lemmatized words\n",
    "print(\"\\n=== Lemmatized Words ===\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85391abe-02ae-433c-a696-821c768048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = nltk.pos_tag(word_tokens)\n",
    "print(\"\\n=== POS Tags ===\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2af1e-8ddd-408d-a8c5-e3cdbe4cf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317cb2b-2b11-4f02-bbfd-255e2b188fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f42d8-8747-4957-9324-a800cd5f4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "    for word, pos in pos_tags\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263509c4-72b4-446e-9438-c3a9e6e32a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display lemmatized words\n",
    "print(\"\\n=== Lemmatized Words ===\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b014a9f-1f3c-44de-ae23-1ae9987d6484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
